import torch
import torch
import json
import os
import numpy as np
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from datasets import load_dataset
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor
from functools import partial
import torch.multiprocessing as mp

def choose_folder_name(percentage_to_remove):
  if percentage_to_remove == 0: return "original"
  elif percentage_to_remove == 0.05: return "5-percent-removed"
  elif percentage_to_remove == 0.10: return "10-percent-removed"
  elif percentage_to_remove == 0.15: return "15-percent-removed"
  elif percentage_to_remove == 0.20: return "20-percent-removed"
  elif percentage_to_remove == 0.25: return "25-percent-removed"

NUM_HYPOTHESES = 5
PERCENTAGE_TO_REMOVE = 0.10
IS_PARTIAL = True if PERCENTAGE_TO_REMOVE > 0.0 else False
# validation.clean, validation.other, test.other, test.clean, train.clean.100, train.clean.360, train.other.500
DS_SPLIT = "train.other.500"
FILE_NAME_PREFIX = f"../../scratch-shared/librispeech-data/{choose_folder_name(PERCENTAGE_TO_REMOVE)}"
OUTPUT_FILE_NAME = f"{FILE_NAME_PREFIX}/{DS_SPLIT.replace('.', '-')}.json"
MAX_WORKERS = 1
CHECKPOINT_INTERVAL = 100
CHECKPOINT_FOLDER = f"{FILE_NAME_PREFIX}/checkpoints"
assert NUM_HYPOTHESES > 0
assert PERCENTAGE_TO_REMOVE >= 0

# Function to load a checkpoint and continue processing
def resume_from_checkpoint(checkpoint_file, data):
  with open(checkpoint_file, 'r') as json_file:
    checkpoint_data = json.load(json_file)
  data.extend(checkpoint_data)
  return data

def load_whisper_model():
  processor = WhisperProcessor.from_pretrained("openai/whisper-small.en")
  model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small.en").to("cuda")
  return processor, model

def generate_partial_audio(audio, percentage_to_remove=PERCENTAGE_TO_REMOVE):
  if percentage_to_remove > 0:
    num_secs = len(audio["array"]) / audio["sampling_rate"]
    num_secs_to_remove = percentage_to_remove * num_secs
    start_secs = num_secs - num_secs_to_remove
    start_pos = int(start_secs * audio["sampling_rate"])
    audio["array"] = audio["array"][:start_pos]

    return audio
  
  return audio

def generate_instruction(num_hypotheses=NUM_HYPOTHESES, is_partial=IS_PARTIAL):
  if is_partial:
    return f"Perform error correction on the top {num_hypotheses} outputs generated by an Automatic Speech Recognition (ASR) system for the provided partial audio segment. The audio was removed from the end of the snippet for this task. The ASR hypotheses, listed in order of their ASR posterior score, are as follows:"
  else:
    return f"Perform error correction on the top {num_hypotheses} outputs generated by an Automatic Speech Recognition (ASR) system. The ASR hypotheses, listed in order of their ASR posterior score, are as follows:"

def process_example(instruction, processor, model, example, num_hypotheses=NUM_HYPOTHESES, percentage_to_remove=PERCENTAGE_TO_REMOVE):
  audio = example["audio"]
  if percentage_to_remove > 0.0:
    audio = generate_partial_audio(audio, percentage_to_remove)
  input_features = processor(audio["array"], sampling_rate=audio["sampling_rate"], return_tensors="pt").input_features
  output = processor.tokenizer._normalize(example["text"])

  with torch.no_grad():
    beam_outputs = model.generate(
      input_features.to("cuda"),
      num_beams=num_hypotheses,
      num_return_sequences=num_hypotheses,
      max_new_tokens=256,
      early_stopping=True
    )

  input_text = ""
  for i, beam_output in enumerate(beam_outputs):
    hypothesis = f"<hypothesis{i + 1}>" + processor.tokenizer._normalize(processor.decode(beam_output, skip_special_tokens=True)) + f"</hypothesis{i + 1}>\n"
    input_text += hypothesis

  input_text += "\nPlease provide the corrected top1 ASR transcription of the given utterance only, do not add any explanations or other words."

  data_point = {
    "instruction": instruction,
    "input": input_text,
    "output": output,
  }

  return data_point

def process_example_with_executor(instruction, processor, model, example):
  return process_example(instruction, processor, model, example)

def main():
  processor, model = load_whisper_model()
  instruction = generate_instruction()

  dataset = load_dataset("librispeech_asr", "all", split=DS_SPLIT, cache_dir="datasets")
  # num_data_points = len(dataset)
  num_data_points = 500

  data = []
  if num_data_points == len(dataset):
    examples = [example for example in dataset]
  else:
    print("HERE")
    examples = []
    for count, example in enumerate(dataset):
      print("count", count)
      if count == num_data_points:
        break
      
      examples.append(example)

  # Check if a checkpoint file exists for resuming
  # all_files = os.listdir(CHECKPOINT_FOLDER)

  # checkpoint_files = [file for file in all_files if file.startswith(f"{CHECKPOINT_FOLDER}/{DS_SPLIT.replace('.', '-')}-checkpoint-") and file.endswith(".json")]
  # checkpoint_files = sorted(checkpoint_files, key=lambda x: int(x.split("checkpoint-")[1].split(".")[0]))

  # if any(os.path.exists(checkpoint_file) for checkpoint_file in checkpoint_files):
  #   for checkpoint_file in checkpoint_files:
  #     if os.path.exists(checkpoint_file):
  #       checkpoint_iteration = int(checkpoint_file.split("checkpoint-")[1].split(".")[0])
  #       print(f"Resuming from checkpoint: {checkpoint_file} at iteration {checkpoint_iteration}...")
  #       data = resume_from_checkpoint(checkpoint_file, data)
  #       examples = examples[checkpoint_iteration:]
  #       num_data_points = len(examples)
  #       print(f"Resized dataset size: {num_data_points}")
  #       break

  if MAX_WORKERS == 1:
    for i, example in enumerate(tqdm(dataset, total=num_data_points)):
      if len(data) == num_data_points:
        break

      data_point = process_example(instruction, processor, model, example)
      data.append(data_point)

      # if i > 0 and i % CHECKPOINT_INTERVAL == 0:
      #   checkpoint_file = f"{CHECKPOINT_FOLDER}/{DS_SPLIT.replace('.', '-')}-checkpoint-{i}.json"
      #   with open(checkpoint_file, 'w') as json_file:
      #     json.dump(data, json_file)
      #   print(f"Checkpoint saved to {checkpoint_file}")

  else:
    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
      process_example_with_executor_partial = partial(process_example_with_executor, instruction, processor, model)

      for i, result in enumerate(tqdm(executor.map(process_example_with_executor_partial, examples), total=num_data_points)):
        data.append(result)

        # if i > 0 and i % CHECKPOINT_INTERVAL == 0:
        #   checkpoint_file = f"{CHECKPOINT_FOLDER}/{DS_SPLIT.replace('.', '-')}-checkpoint-{i}.json"
        #   with open(checkpoint_file, 'w') as json_file:
        #     json.dump(data, json_file)
        #   print(f"Checkpoint saved to {checkpoint_file}")

  with open(OUTPUT_FILE_NAME, 'w') as json_file:
    json.dump(data, json_file)

  # Remove any existing checkpoint files
  # for i in range(0, num_data_points, CHECKPOINT_INTERVAL):
  #   checkpoint_file = f"{CHECKPOINT_FOLDER}/{DS_SPLIT.replace('.', '-')}-checkpoint-{i}.json"
  #   if os.path.exists(checkpoint_file):
  #     os.remove(checkpoint_file)

if __name__ == "__main__":
  if MAX_WORKERS > 1:
    mp.set_start_method('spawn')
  main()
