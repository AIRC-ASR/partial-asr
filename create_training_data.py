import torch
import json
import random
import os
import numpy as np
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from datasets import load_dataset
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor
from functools import partial
import torch.multiprocessing as mp

NUM_HYPOTHESES = 5
PERCENTAGE_TO_REMOVE = 0.25
IS_PARTIAL = True if PERCENTAGE_TO_REMOVE > 0.0 else False
FILE_NAME_PREFIX = "/gpfs/u/home/NLUG/NLUGcbsm/scratch-shared/librispeech-data/25-percent-removed/train-clean"
OUTPUT_FILE_NAME = f"{FILE_NAME_PREFIX}-360.json"
DS_SPLIT = "train.clean.360"
MAX_WORKERS = 10
BATCH_SIZE = 10
CHECKPOINT_INTERVAL = 100
assert NUM_HYPOTHESES > 0
assert PERCENTAGE_TO_REMOVE >= 0

# Function to load a checkpoint and continue processing
def resume_from_checkpoint(checkpoint_file, data):
  with open(checkpoint_file, 'r') as json_file:
    checkpoint_data = json.load(json_file)
  data.extend(checkpoint_data)
  return data

def load_whisper_model():
  processor = WhisperProcessor.from_pretrained("openai/whisper-small.en")
  model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small.en").to("cuda")
  return processor, model

def generate_partial_audio(audio, percentage_to_remove=PERCENTAGE_TO_REMOVE):
  num_secs = len(audio["array"]) / audio["sampling_rate"]

  if percentage_to_remove > 0:
    num_secs_to_remove = percentage_to_remove * num_secs
    start_secs = random.uniform(num_secs_to_remove, num_secs - num_secs_to_remove)
    end_secs = start_secs + num_secs_to_remove
    start_pos = int(start_secs * audio["sampling_rate"])
    end_pos = int(end_secs * audio["sampling_rate"])

    beginning_audio = audio["array"][:start_pos]
    ending_audio = audio["array"][end_pos:]
    partial_audio = np.concatenate((beginning_audio, ending_audio))
    return partial_audio

def generate_instruction(num_hypotheses=NUM_HYPOTHESES, is_partial=IS_PARTIAL):
  if is_partial:
    return f"Perform error correction on the top {num_hypotheses} outputs generated by an Automatic Speech Recognition (ASR) system for the provided partial audio segment. The ASR hypotheses, listed in order of their ASR posterior score, are as follows:"
  else:
    return f"Perform error correction on the top {num_hypotheses} outputs generated by an Automatic Speech Recognition (ASR) system. The ASR hypotheses, listed in order of their ASR posterior score, are as follows:"

def process_example(instruction, processor, model, example, num_hypotheses=NUM_HYPOTHESES, percentage_to_remove=PERCENTAGE_TO_REMOVE):
  audio = example["audio"]
  partial_audio = generate_partial_audio(audio, percentage_to_remove)
  input_features = processor(partial_audio, sampling_rate=audio["sampling_rate"], return_tensors="pt").input_features
  output = processor.tokenizer._normalize(example["text"])

  with torch.no_grad():
    beam_outputs = model.generate(
      input_features.to("cuda"),
      num_beams=num_hypotheses,
      num_return_sequences=num_hypotheses,
      max_new_tokens=256,
      early_stopping=True
    )

  input_text = ""
  for i, beam_output in enumerate(beam_outputs):
    hypothesis = f"<hypothesis{i + 1}>" + processor.tokenizer._normalize(processor.decode(beam_output, skip_special_tokens=True)) + f"</hypothesis{i + 1}>\n"
    input_text += hypothesis

  input_text += "\nPlease provide the corrected top1 ASR transcription of the given utterance only, do not add any explanations or other words."

  data_point = {
    "instruction": instruction,
    "input": input_text,
    "output": output,
  }

  return data_point

def process_example_with_executor(instruction, processor, model, example):
  return process_example(instruction, processor, model, example)

def main():
  processor, model = load_whisper_model()
  instruction = generate_instruction()

  dataset = load_dataset("librispeech_asr", "all", split=DS_SPLIT, cache_dir="datasets")
  num_data_points = len(dataset)

  data = []
  examples = [example for example in dataset]

  # Check if a checkpoint file exists for resuming
  checkpoint_files = [f"{FILE_NAME_PREFIX}-checkpoint-{i}.json" for i in range(0, num_data_points, CHECKPOINT_INTERVAL)][::-1]
  print('checkpoint_files', checkpoint_files)
  if any(os.path.exists(checkpoint_file) for checkpoint_file in checkpoint_files):
    data = []  # Initialize an empty list to accumulate results
    for checkpoint_file in checkpoint_files:
      if os.path.exists(checkpoint_file):
        checkpoint_iteration = int(checkpoint_file.split("checkpoint-")[1].split(".")[0])
        print(f"Resuming from checkpoint: {checkpoint_file} at iteration {checkpoint_iteration}...")
        data = resume_from_checkpoint(checkpoint_file, data)
        examples = examples[checkpoint_iteration:]
        num_data_points = len(examples)
        print(f"Resized dataset size: {num_data_points}")
        break

    for checkpoint_file in checkpoint_files:
      os.remove(checkpoint_file)  # Remove the loaded checkpoint

  if MAX_WORKERS == 1:
    for example in tqdm(dataset, total=num_data_points):
      if len(data) == num_data_points:
        break

      data_point = process_example(instruction, processor, model, example)
      data.append(data_point)

      if i > 0 and i % CHECKPOINT_INTERVAL == 0:
        checkpoint_file = f"{FILE_NAME_PREFIX}-checkpoint-{i}.json"
        with open(checkpoint_file, 'w') as json_file:
          json.dump(data, json_file)
        print(f"Checkpoint saved to {checkpoint_file}")

  else:
    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:
      process_example_with_executor_partial = partial(process_example_with_executor, instruction, processor, model)

      for i, result in enumerate(tqdm(executor.map(process_example_with_executor_partial, examples), total=num_data_points)):
        data.append(result)

        if i > 0 and i % CHECKPOINT_INTERVAL == 0:
          checkpoint_file = f"{FILE_NAME_PREFIX}-checkpoint-{i}.json"
          with open(checkpoint_file, 'w') as json_file:
            json.dump(data, json_file)
          print(f"Checkpoint saved to {checkpoint_file}")

  with open(OUTPUT_FILE_NAME, 'w') as json_file:
    json.dump(data, json_file)

  # Remove any existing checkpoint files
  for i in range(0, num_data_points, CHECKPOINT_INTERVAL):
    checkpoint_file = f"{FILE_NAME_PREFIX}-checkpoint-{i}.json"
    if os.path.exists(checkpoint_file):
      os.remove(checkpoint_file)

if __name__ == "__main__":
  mp.set_start_method('spawn')
  main()
