import torch
import json
import random
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from datasets import load_dataset
from tqdm import tqdm
import numpy as np

NUM_HYPOTHESES = 5
PERCENTAGE_TO_REMOVE = 0.25
IS_PARTIAL = True if PERCENTAGE_TO_REMOVE > 0.0 else False
OUTPUT_FILE_NAME = "/gpfs/u/home/NLUG/NLUGcbsm/scratch-shared/librispeech-data/25-percent-removed/train-clean-100.json"
DS_SPLIT = "train.clean.100"
assert NUM_HYPOTHESES > 0
assert PERCENTAGE_TO_REMOVE >= 0

def load_whisper_model():
  processor = WhisperProcessor.from_pretrained("openai/whisper-small.en")
  model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small.en").to("cuda")
  return processor, model

def generate_partial_audio(audio, percentage_to_remove=PERCENTAGE_TO_REMOVE):
  num_secs = len(audio["array"]) / audio["sampling_rate"]

  if percentage_to_remove > 0:
    num_secs_to_remove = percentage_to_remove * num_secs
    start_secs = random.uniform(num_secs_to_remove, num_secs - num_secs_to_remove)
    end_secs = start_secs + num_secs_to_remove
    start_pos = int(start_secs * audio["sampling_rate"])
    end_pos = int(end_secs * audio["sampling_rate"])

    beginning_audio = audio["array"][:start_pos]
    ending_audio = audio["array"][end_pos:]
    partial_audio = np.concatenate((beginning_audio, ending_audio))
    return partial_audio

def generate_instruction(num_hypotheses=NUM_HYPOTHESES, is_partial=IS_PARTIAL):
  if is_partial:
    return f"Perform error correction on the top {num_hypotheses} outputs generated by an Automatic Speech Recognition (ASR) system for the provided partial audio segment. The ASR hypotheses, listed in order of their ASR posterior score, are as follows:"
  else:
    return f"Perform error correction on the top {num_hypotheses} outputs generated by an Automatic Speech Recognition (ASR) system. The ASR hypotheses, listed in order of their ASR posterior score, are as follows:"

def process_example(processor, model, example, num_hypotheses=NUM_HYPOTHESES, percentage_to_remove=PERCENTAGE_TO_REMOVE):
  audio = example["audio"]
  partial_audio = generate_partial_audio(audio, percentage_to_remove)
  input_features = processor(partial_audio, sampling_rate=audio["sampling_rate"], return_tensors="pt").input_features
  output = processor.tokenizer._normalize(example["text"])

  with torch.no_grad():
    beam_outputs = model.generate(
      input_features.to("cuda"),
      num_beams=num_hypotheses,
      num_return_sequences=num_hypotheses,
      max_new_tokens=256,
      early_stopping=True
    )

  input_text = ""
  for i, beam_output in enumerate(beam_outputs):
    hypothesis = f"<hypothesis{i + 1}>" + processor.tokenizer._normalize(processor.decode(beam_output, skip_special_tokens=True)) + f"</hypothesis{i + 1}>\n"
    input_text += hypothesis

  input_text += "\nPlease provide the corrected top1 ASR transcription of the given utterance only, do not add any explanations or other words."

  return input_text, output

def main():
  processor, model = load_whisper_model()
  instruction = generate_instruction()

  dataset = load_dataset("librispeech_asr", "all", split=DS_SPLIT, cache_dir="datasets")
  num_data_points = len(dataset)
  data = []

  for example in tqdm(dataset, total=num_data_points):
    if len(data) == num_data_points:
      break

    input_text, output = process_example(processor, model, example)
    data_point = {
      "instruction": instruction,
      "input": input_text,
      "output": output,
    }
    data.append(data_point)

  with open(OUTPUT_FILE_NAME, 'w') as json_file:
    json.dump(data, json_file)

if __name__ == "__main__":
    main()
