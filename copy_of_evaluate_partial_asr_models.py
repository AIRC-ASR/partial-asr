# -*- coding: utf-8 -*-
"""Copy of Evaluate Partial ASR Models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I-zdIby0bQNjDLjPvgR4Wb1U0sBeaCpT

# Setup
"""


import torch
import pickle
from evaluate import load
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM
from transformers import AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm
from transformers import WhisperProcessor

# Load the metric functions
wer = load("wer")
# cer = load("cer")

# torch.cuda.set_device(2)
# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained("TinyPixel/Llama-2-7B-bf16-sharded", trust_remote_code=True, device_map="auto", cache_dir="models")
tokenizer.pad_token = tokenizer.eos_token

from transformers import GenerationConfig

device = "cuda" if torch.cuda.is_available() else "cpu"

def evaluate(
    prompt,
    model,
    tokenizer,
    input=None,
    temperature=0.7,
    num_beams=5,
    max_new_tokens=128,
    **kwargs,
):
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids = inputs["input_ids"].to(device)
    generation_config = GenerationConfig(
        num_beams=num_beams,
        **kwargs,
    )
    generate_params = {
        "input_ids": input_ids,
        "generation_config": generation_config,
        "return_dict_in_generate": False,
        "output_scores": False,
        "max_new_tokens": max_new_tokens,
    }

    with torch.no_grad():
        generation_output = model.generate(
            input_ids=input_ids,
            generation_config=generation_config,
            return_dict_in_generate=False,
            output_scores=False,
            max_new_tokens=max_new_tokens,
            early_stopping=True,
            use_cache=True,
            num_return_sequences=1,
            no_repeat_ngram_size = 2

        )
    s = generation_output[0]
    output = tokenizer.decode(s)
    output = output.replace("<s>", "").replace("</s>", "")
    if "###Assistant" in output:
        output = output.split("###Assistant")[1]
        print("OUT", output)

    yield output

def find_model_predictions(prompts, model, tokenizer):
    predictions = []
    processor = WhisperProcessor.from_pretrained("openai/whisper-small.en")
    # with open("baseline_test_other_1300_predictions.pkl", "rb") as f:
    #     predictions = pickle.load(f)
    # print("LEN", len(predictions))
    i = 0
    for prompt in tqdm(prompts):
        if i < len(predictions):
            i += 1
            continue
        for model_output in evaluate(prompt, model, tokenizer):
            model_output = processor.tokenizer._normalize(model_output)
            print(i, model_output)
            predictions.append(model_output)
            if i % 100 == 0:
                with open(f"five_percent_test_other_{i}_predictions.pkl", "wb") as f:
                    pickle.dump(predictions, f)

        i += 1

    with open(f"five_percent_test_other_predictions.pkl", "wb") as f:
        pickle.dump(predictions, f)
    return predictions

"""# Run baseline benchmarks"""

# Get the test other and test clean datasets for original audio
# Get just the outputs in their own list for all of them
baseline_test_other_ds = load_dataset("json", data_files="llama2-data-eval/original/test-other.json")["train"]
baseline_test_clean_ds = load_dataset("json", data_files="llama2-data-eval/original/test-clean.json")["train"]
baseline_test_other_prompts = baseline_test_other_ds["text"]
baseline_test_clean_prompts = baseline_test_clean_ds["text"]
baseline_test_other_refs = baseline_test_other_ds["output"]
baseline_test_clean_refs = baseline_test_clean_ds["output"]

# Get the baseline model
# baseline_model = AutoModelForCausalLM.from_pretrained("TinyPixel/Llama-2-7B-bf16-sharded", load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, device_map="auto", cache_dir="models")
# baseline_model.eval()

# print("FINDING WER For Librispeech Test Other (BASELINE)")
# baseline_test_other_preds = find_model_predictions(baseline_test_other_prompts, baseline_model, tokenizer)
# baseline_test_other_wer = 100 * wer.compute(predictions=baseline_test_other_preds, references=baseline_test_other_refs)
# print(f"LibriSpeech Test Other WER (BASELINE): {baseline_test_other_wer:.2f}%")

# print("FINDING WER For Librispeech Test Clean (BASELINE)")
# baseline_test_clean_preds = find_model_predictions(baseline_test_clean_prompts, baseline_model, tokenizer)
# baseline_test_clean_wer = 100 * wer.compute(predictions=baseline_test_clean_preds, references=baseline_test_clean_refs)
# print(f"LibriSpeech Test Clean WER (BASELINE): {baseline_test_clean_wer:.2f}%")

# Get the test other and test clean datasets for original audio
original_test_other_ds = baseline_test_other_ds[:]
original_test_clean_ds = baseline_test_clean_ds[:]
original_test_other_prompts = baseline_test_other_prompts[:]
original_test_clean_prompts = baseline_test_clean_prompts[:]
original_test_other_refs = baseline_test_other_refs[:]
original_test_clean_refs = baseline_test_clean_refs[:]

# Get the instruction tuned models for original audio file
original_config = PeftConfig.from_pretrained("omarc/llama2-partial-asr-original")
original_model = AutoModelForCausalLM.from_pretrained("TinyPixel/Llama-2-7B-bf16-sharded", load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, device_map={"":1}, cache_dir="models")
original_model = PeftModel.from_pretrained(original_model, "omarc/llama2-partial-asr-original", config=original_config, cache_dir="models")
original_model.eval()

# print("FINDING WER For Librispeech Test Other (ORIGINAL)")
original_test_other_preds = find_model_predictions(original_test_other_prompts, original_model, tokenizer)
original_test_other_wer = 100 * wer.compute(predictions=original_test_other_preds, references=original_test_other_refs)
print(f"LibriSpeech Test Other WER (ORIGINAL): {original_test_other_wer:.2f}%")

# print("FINDING WER For Librispeech Test Clean (ORIGINAL)")
# original_test_clean_preds = find_model_predictions(original_test_clean_prompts, original_model, tokenizer)
# original_test_clean_wer = 100 * wer.compute(predictions=original_test_clean_preds, references=original_test_clean_refs)
# print(f"LibriSpeech Test Clean WER (ORIGINAL): {original_test_clean_wer:.2f}%")

# Get the test other and test clean datasets for 5% audio removed
# five_percent_test_other_ds = load_dataset("json", data_files="llama2-data-eval/5-percent-removed/test-other.jsonl")["train"]
# five_percent_test_clean_ds = load_dataset("json", data_files="llama2-data-eval/5-percent-removed/test-clean.jsonl")["train"]
# five_percent_test_other_prompts = five_percent_test_other_ds["text"]
# five_percent_test_clean_prompts = five_percent_test_clean_ds["text"]
# five_percent_test_other_refs = five_percent_test_other_ds["output"]
# five_percent_test_clean_refs = five_percent_test_clean_ds["output"]

# # # Get the instruction tuned models for original audio file
# five_percent_config = PeftConfig.from_pretrained("omarc/llama2-partial-asr-5-percent-removed")
# five_percent_model = AutoModelForCausalLM.from_pretrained("TinyPixel/Llama-2-7B-bf16-sharded", load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, cache_dir="models")
# five_percent_model = PeftModel.from_pretrained(five_percent_model, "omarc/llama2-partial-asr-5-percent-removed", config=five_percent_config, cache_dir="models")
# five_percent_model.eval()

# print("FINDING WER For Librispeech Test Other (5%)")
# five_percent_test_other_preds = find_model_predictions(five_percent_test_other_prompts, five_percent_model, tokenizer)
# five_percent_test_other_wer = 100 * wer.compute(predictions=five_percent_test_other_preds, references=five_percent_test_other_refs)
# # five_percent_test_other_cer = 100 * cer.compute(predictions=five_percent_test_other_preds, references=five_percent_test_other_refs)
# print(f"LibriSpeech Test Other WER (5%): {five_percent_test_other_wer:.2f}%")
# print(f"LibriSpeech Test Other CER (5%): {five_percent_test_other_cer:.2f}%")

# print("FINDING WER For Librispeech Test Clean (5%)")
# five_percent_test_clean_preds = find_model_predictions(five_percent_test_clean_prompts, five_percent_model, tokenizer)
# five_percent_test_clean_wer = 100 * wer.compute(predictions=five_percent_test_clean_preds, references=five_percent_test_clean_refs)
# five_percent_test_clean_cer = 100 * cer.compute(predictions=five_percent_test_clean_preds, references=five_percent_test_clean_refs)
# print(f"LibriSpeech Test Clean WER (5%): {five_percent_test_clean_wer:.2f}%")
# print(f"LibriSpeech Test Clean CER (5%): {five_percent_test_clean_cer:.2f}%")

